{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find Apache Spark on this machine\n",
    "import findspark\n",
    "findspark.init('/Users/giacomogregori/spark')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Build a Spark SQL Session for DataFrames\n",
    "master = 'local[2]'\n",
    "appName = 'Weather delays percentages'\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(appName) \\\n",
    "    .master(master) \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "full_data = '../dataset/*.csv.bz2'\n",
    "full_data_parquet = '../dataset/RITA_1994-2008.parquet'\n",
    "\n",
    "path = Path(full_data_parquet)\n",
    "# If reduced dataset is not found, load the full compressed dataset and reduce it.\n",
    "# This is going to take lot of time. Just wait.\n",
    "if not path.is_dir():\n",
    "    df = spark.read.csv(full_data, inferSchema=True, header=True, sep=',')\n",
    "    df.replace('NA', None) \\\n",
    "    .write \\\n",
    "    .save(full_data_parquet, format='parquet')\n",
    "\n",
    "# Load the reduced dataset\n",
    "df = spark.read.load(full_data_parquet, format='parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Year',\n",
       " 'Month',\n",
       " 'DayofMonth',\n",
       " 'DayOfWeek',\n",
       " 'DepTime',\n",
       " 'CRSDepTime',\n",
       " 'ArrTime',\n",
       " 'CRSArrTime',\n",
       " 'UniqueCarrier',\n",
       " 'FlightNum',\n",
       " 'TailNum',\n",
       " 'ActualElapsedTime',\n",
       " 'CRSElapsedTime',\n",
       " 'AirTime',\n",
       " 'ArrDelay',\n",
       " 'DepDelay',\n",
       " 'Origin',\n",
       " 'Dest',\n",
       " 'Distance',\n",
       " 'TaxiIn',\n",
       " 'TaxiOut',\n",
       " 'Cancelled',\n",
       " 'CancellationCode',\n",
       " 'Diverted',\n",
       " 'CarrierDelay',\n",
       " 'WeatherDelay',\n",
       " 'NASDelay',\n",
       " 'SecurityDelay',\n",
       " 'LateAircraftDelay']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+--------+------------+------------+--------+-------------+-----------------+\n",
      "|               Date|WeekYear|CarrierDelay|WeatherDelay|NASDelay|SecurityDelay|LateAircraftDelay|\n",
      "+-------------------+--------+------------+------------+--------+-------------+-----------------+\n",
      "|2007-01-01 00:00:00|       1|           0|           0|       0|            0|                0|\n",
      "|2007-01-01 00:00:00|       1|           0|           0|       0|            0|                0|\n",
      "|2007-01-01 00:00:00|       1|           3|           0|       0|            0|               31|\n",
      "|2007-01-01 00:00:00|       1|          23|           0|       0|            0|                3|\n",
      "|2007-01-01 00:00:00|       1|           0|           0|       0|            0|                0|\n",
      "|2007-01-01 00:00:00|       1|           0|           0|       0|            0|                0|\n",
      "|2007-01-01 00:00:00|       1|          46|           0|       0|            0|                1|\n",
      "|2007-01-01 00:00:00|       1|           0|           0|       0|            0|                0|\n",
      "|2007-01-01 00:00:00|       1|          20|           0|       0|            0|               24|\n",
      "|2007-01-01 00:00:00|       1|           0|           0|       0|            0|                0|\n",
      "+-------------------+--------+------------+------------+--------+-------------+-----------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Drop cancelled flights\n",
    "df = df.drop(df['Cancelled'] == 1)\n",
    "\n",
    "\n",
    "# Parse dates to datetime format\n",
    "import datetime\n",
    "import pyspark.sql.functions as F\n",
    "from pyspark.sql.types import TimestampType, IntegerType\n",
    "\n",
    "make_date = lambda year, month, day : datetime.datetime(year, month, day) \n",
    "make_date = F.udf(make_date, TimestampType())\n",
    "\n",
    "week_year = lambda date : date.isocalendar()[1]\n",
    "week_year = F.udf(week_year, IntegerType())\n",
    "\n",
    "df = df.select(make_date(df['Year'], df['Month'], df['DayofMonth']).alias('Date'), \\\n",
    "               'DayOfWeek', 'CarrierDelay', 'WeatherDelay','NASDelay','SecurityDelay','LateAircraftDelay')\n",
    "df = df.select('Date', week_year('Date').alias('WeekYear'), 'CarrierDelay', 'WeatherDelay','NASDelay','SecurityDelay','LateAircraftDelay')\n",
    "df.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+--------+-------------+\n",
      "|Year|WeekYear|weather_count|\n",
      "+----+--------+-------------+\n",
      "|2008|      35|         1413|\n",
      "|2005|      29|         3018|\n",
      "|2005|      49|         3513|\n",
      "|2007|       6|         2165|\n",
      "|2007|      52|         3271|\n",
      "|2005|       5|         1267|\n",
      "|2007|      28|         2940|\n",
      "|2005|      51|         1584|\n",
      "|2005|      25|         1653|\n",
      "|2008|      28|         3249|\n",
      "+----+--------+-------------+\n",
      "only showing top 10 rows\n",
      "\n",
      "+----+--------+-------------+\n",
      "|Year|WeekYear|general_count|\n",
      "+----+--------+-------------+\n",
      "|2008|      35|        19977|\n",
      "|2005|      29|        37064|\n",
      "|2005|      49|        34575|\n",
      "|2007|       6|        31743|\n",
      "|2007|      52|        47949|\n",
      "|2005|       5|        19279|\n",
      "|2007|      28|        42737|\n",
      "|2005|      51|        32616|\n",
      "|2005|      25|        26598|\n",
      "|2008|      28|        39097|\n",
      "+----+--------+-------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Flights that have a WeatherDelay\n",
    "weather_delayed_flights = df.filter(df['WeatherDelay'] > 0)\n",
    "\n",
    "# Flights that have a Delay\n",
    "delayed_flights = df.filter((df['CarrierDelay'] > 0) | (df['WeatherDelay'] > 0) | (df['NASDelay'] > 0) | (df['SecurityDelay'] > 0) | (df['LateAircraftDelay'] > 0))\n",
    "\n",
    "# Number of times per week flights had a weather delay or a general delay  \n",
    "weather_delays = weather_delayed_flights.groupBy([F.year('Date').alias('Year'), 'WeekYear']).count()\n",
    "general_delays = delayed_flights.groupBy([F.year('Date').alias('Year'), 'WeekYear']).count()\n",
    "                            \n",
    "weather_delays = weather_delays.select('Year', 'WeekYear', weather_delays['count'].alias('weather_count'))\n",
    "general_delays = general_delays.select('Year', 'WeekYear', general_delays['count'].alias('general_count'))\n",
    "\n",
    "                            \n",
    "weather_delays.show(10)\n",
    "general_delays.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+--------+-------------+-------------+\n",
      "|Year|WeekYear|weather_count|general_count|\n",
      "+----+--------+-------------+-------------+\n",
      "|2008|      35|         1413|        19977|\n",
      "|2005|      29|         3018|        37064|\n",
      "|2005|      49|         3513|        34575|\n",
      "|2005|       5|         1267|        19279|\n",
      "|2007|       6|         2165|        31743|\n",
      "|2007|      52|         3271|        47949|\n",
      "|2007|      28|         2940|        42737|\n",
      "|2005|      51|         1584|        32616|\n",
      "|2005|      25|         1653|        26598|\n",
      "|2005|      22|         2670|        26607|\n",
      "+----+--------+-------------+-------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Calculate the percentage\n",
    "percentage_weather_delays = weather_delays \\\n",
    "                .join(general_delays, [\"Year\",\"WeekYear\"])\n",
    "    \n",
    "percentage_weather_delays.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+--------+-------------+----+--------+-------------+--------------------+\n",
      "|Year|WeekYear|weather_count|Year|WeekYear|general_count| WeeklyWeatherDelays|\n",
      "+----+--------+-------------+----+--------+-------------+--------------------+\n",
      "|2008|      35|         1413|2008|      35|        19977| 0.07073134104219853|\n",
      "|2005|      29|         3018|2005|      29|        37064| 0.08142672134685949|\n",
      "|2005|      49|         3513|2005|      49|        34575| 0.10160520607375272|\n",
      "|2005|       5|         1267|2005|       5|        19279|   0.065719176305825|\n",
      "|2007|       6|         2165|2007|       6|        31743| 0.06820401348328765|\n",
      "|2007|      52|         3271|2007|      52|        47949|  0.0682183152933325|\n",
      "|2007|      28|         2940|2007|      28|        42737| 0.06879284928750264|\n",
      "|2005|      51|         1584|2005|      51|        32616| 0.04856512141280353|\n",
      "|2005|      25|         1653|2005|      25|        26598|0.062147529889465376|\n",
      "|2005|      22|         2670|2005|      22|        26607| 0.10034953207802458|\n",
      "+----+--------+-------------+----+--------+-------------+--------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "percentage_weather_delays = percentage_weather_delays.withColumn(\"WeeklyWeatherDelays\", (F.col(\"weather_count\") / F.col(\"general_count\")))\n",
    "#['WeeklyWeatherDelays']= percentage_weather_delays['weather_count'] / percentage_weather_delays['general_count']\n",
    "percentage_weather_delays.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "ename": "AnalysisException",
     "evalue": "\"Reference 'Year' is ambiguous, could be: Year, Year.;\"",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m~/spark/python/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/spark/python/lib/py4j-0.10.7-src.zip/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    327\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 328\u001b[0;31m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[1;32m    329\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o333.select.\n: org.apache.spark.sql.AnalysisException: Reference 'Year' is ambiguous, could be: Year, Year.;\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolve(LogicalPlan.scala:213)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveChildren(LogicalPlan.scala:97)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveReferences$$anonfun$36.apply(Analyzer.scala:822)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveReferences$$anonfun$36.apply(Analyzer.scala:824)\n\tat org.apache.spark.sql.catalyst.analysis.package$.withPosition(package.scala:53)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveReferences$.org$apache$spark$sql$catalyst$analysis$Analyzer$ResolveReferences$$resolve(Analyzer.scala:821)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveReferences$$anonfun$org$apache$spark$sql$catalyst$analysis$Analyzer$ResolveReferences$$resolve$2.apply(Analyzer.scala:830)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveReferences$$anonfun$org$apache$spark$sql$catalyst$analysis$Analyzer$ResolveReferences$$resolve$2.apply(Analyzer.scala:830)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$4.apply(TreeNode.scala:306)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:187)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.mapChildren(TreeNode.scala:304)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveReferences$.org$apache$spark$sql$catalyst$analysis$Analyzer$ResolveReferences$$resolve(Analyzer.scala:830)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveReferences$$anonfun$apply$9$$anonfun$applyOrElse$36.apply(Analyzer.scala:891)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveReferences$$anonfun$apply$9$$anonfun$applyOrElse$36.apply(Analyzer.scala:891)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan$$anonfun$1.apply(QueryPlan.scala:107)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan$$anonfun$1.apply(QueryPlan.scala:107)\n\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:70)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan.transformExpression$1(QueryPlan.scala:106)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan.org$apache$spark$sql$catalyst$plans$QueryPlan$$recursiveTransform$1(QueryPlan.scala:118)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan$$anonfun$org$apache$spark$sql$catalyst$plans$QueryPlan$$recursiveTransform$1$1.apply(QueryPlan.scala:122)\n\tat scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)\n\tat scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)\n\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\n\tat scala.collection.TraversableLike$class.map(TraversableLike.scala:234)\n\tat scala.collection.AbstractTraversable.map(Traversable.scala:104)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan.org$apache$spark$sql$catalyst$plans$QueryPlan$$recursiveTransform$1(QueryPlan.scala:122)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan$$anonfun$2.apply(QueryPlan.scala:127)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:187)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan.mapExpressions(QueryPlan.scala:127)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveReferences$$anonfun$apply$9.applyOrElse(Analyzer.scala:891)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveReferences$$anonfun$apply$9.applyOrElse(Analyzer.scala:833)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$transformUp$1.apply(TreeNode.scala:289)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$transformUp$1.apply(TreeNode.scala:289)\n\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:70)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformUp(TreeNode.scala:288)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveReferences$.apply(Analyzer.scala:833)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveReferences$.apply(Analyzer.scala:690)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor$$anonfun$execute$1$$anonfun$apply$1.apply(RuleExecutor.scala:87)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor$$anonfun$execute$1$$anonfun$apply$1.apply(RuleExecutor.scala:84)\n\tat scala.collection.LinearSeqOptimized$class.foldLeft(LinearSeqOptimized.scala:124)\n\tat scala.collection.immutable.List.foldLeft(List.scala:84)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor$$anonfun$execute$1.apply(RuleExecutor.scala:84)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor$$anonfun$execute$1.apply(RuleExecutor.scala:76)\n\tat scala.collection.immutable.List.foreach(List.scala:381)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:76)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.org$apache$spark$sql$catalyst$analysis$Analyzer$$executeSameContext(Analyzer.scala:124)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:118)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:103)\n\tat org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:57)\n\tat org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:55)\n\tat org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:47)\n\tat org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:74)\n\tat org.apache.spark.sql.Dataset.org$apache$spark$sql$Dataset$$withPlan(Dataset.scala:3296)\n\tat org.apache.spark.sql.Dataset.select(Dataset.scala:1307)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:748)\n",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-29-5aead1407bd9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mpercentage_weather_delays\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mselect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcol\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Year'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'WeekYear'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'WeeklyWeatherDelays'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/spark/python/pyspark/sql/dataframe.py\u001b[0m in \u001b[0;36mselect\u001b[0;34m(self, *cols)\u001b[0m\n\u001b[1;32m   1200\u001b[0m         \u001b[0;34m[\u001b[0m\u001b[0mRow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34mu'Alice'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mage\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m12\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mRow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34mu'Bob'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mage\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m15\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1201\u001b[0m         \"\"\"\n\u001b[0;32m-> 1202\u001b[0;31m         \u001b[0mjdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mselect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jcols\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mcols\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1203\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjdf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msql_ctx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1204\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/spark/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1255\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1256\u001b[0m         return_value = get_return_value(\n\u001b[0;32m-> 1257\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m   1258\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1259\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/spark/python/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m     67\u001b[0m                                              e.java_exception.getStackTrace()))\n\u001b[1;32m     68\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'org.apache.spark.sql.AnalysisException: '\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 69\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mAnalysisException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m': '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstackTrace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     70\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'org.apache.spark.sql.catalyst.analysis'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mAnalysisException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m': '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstackTrace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAnalysisException\u001b[0m: \"Reference 'Year' is ambiguous, could be: Year, Year.;\""
     ]
    }
   ],
   "source": [
    "\n",
    "percentage_weather_delays.select(F.col('Year'),'WeekYear','WeeklyWeatherDelays')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store output Dataframe (or load it if already existing)\n",
    "final_dataset = '../dataset/weather_analitics.parquet'\n",
    "\n",
    "path= Path(final_dataset)\n",
    "if not path.is_dir():\n",
    "    percentage_weather_delays.write.mode('overwrite').save(final_dataset, format='parquet')\n",
    "else:\n",
    "    percentage_weather_delays = spark.read.load(final_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Output a list of tuples of schema:\n",
    "# ('Year', 'WeekYear', 'Percentage')\n",
    "weather_data = percentage_weather_delays.rdd.map(tuple).collect()\n",
    "print(weather_data[:100])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hide warnings if there are any\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "%matplotlib ipympl\n",
    "\n",
    "from __future__ import print_function\n",
    "from ipywidgets import interact, interactive, fixed, interact_manual\n",
    "import ipywidgets as widgets\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pd_dataframe( years, df):\n",
    "    rows = df.filter(F.col('Year').isin(*years)) \\\n",
    "             .select('Year', 'WeekYear', 'WeeklyWeatherDelays') \\\n",
    "             .orderBy('Year', 'WeekYear') \\\n",
    "             .collect()\n",
    "    \n",
    "    nb_years = len(years)\n",
    "    nb_weeks = 52\n",
    "    data = np.zeros((nb_weeks, nb_years))\n",
    "    for row in rows:\n",
    "        year = row[0] - years[0]\n",
    "        week = row[1] - 1\n",
    "        per = row[2]\n",
    "\n",
    "        if week > 51: continue\n",
    "        data[week, year] = per\n",
    "    columns = [str(y) for y in years]\n",
    "    indices = range(1, 53)\n",
    "    res = pd.DataFrame(data=data, columns=columns, index=indices)\n",
    "    return res\n",
    "\n",
    "def plot_weather_time_series(years, df):\n",
    "    df = get_pd_dataframe(years, df)\n",
    "    title = 'Weekly weather delays percentage'\n",
    "    if df.empty:\n",
    "        print('No data')\n",
    "    else:\n",
    "        df.plot(title=title, grid=True, xticks=range(0, 53, 4), colormap='tab20c')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_average_df(years, df):\n",
    "    rows = df.filter(F.col('Year').isin(*years)) \\\n",
    "             .groupBy('Year') \\\n",
    "             .avg('WeeklyWeatherDelays') \\\n",
    "             .withColumnRenamed('avg(WeeklyWeatherDelays)', 'AverageWeatherDelaysPercentage') \\\n",
    "             .select('Year', 'AverageWeatherDelaysPercentage') \\\n",
    "             .collect()\n",
    "    \n",
    "    nb_years = len(years)\n",
    "    data = np.zeros(nb_years)\n",
    "    for row in rows:\n",
    "        year = row[0] - years[0]\n",
    "        avg_pen = row[1]\n",
    "        data[year] = avg_pen \n",
    "    res = pd.DataFrame({airport: data}, index=years)\n",
    "    return res\n",
    "\n",
    "def plot_average_penalty(airport, years, df):\n",
    "    df = get_average_df(airport, years, df)\n",
    "    title = '{} Airport - Average Penalties'.format(airport)\n",
    "    if df.empty:\n",
    "        print('No data for airport {}'.format(airport))\n",
    "    else:\n",
    "        df.plot.bar(y=airport, title=title, rot=0)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
