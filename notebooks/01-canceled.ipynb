{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize PySpark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find Apache Spark on this machine\n",
    "import findspark\n",
    "findspark.init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Dev mode: False when performing real analytics\n",
    "DEV = True\n",
    "\n",
    "# Build a Spark SQL Session for DataFrames\n",
    "master = 'local[2]'\n",
    "appName = 'Cancelled flights percentages'\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(appName) \\\n",
    "    .master(master) \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------- DEV mode ON ---------\n",
      "Starting preprocessing of ../dataset/1994.csv.bz2\n",
      "Preprocessing NOT performed.\n",
      "Preprocessed dataset already exists: ../dataset/preprocessed_dataset_1994.parquet\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from preprocessing_utils import *\n",
    "if DEV:\n",
    "    # DEV preprocessing\n",
    "    perform_DEV_dataset_preprocessing(spark)\n",
    "else:\n",
    "    # Production preprocessing\n",
    "    perform_dataset_preprocessing(spark)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------- DEV mode ON ---------\n",
      "Peprocessed dataset loaded.\n",
      "../dataset/preprocessed_dataset_1994.parquet\n"
     ]
    }
   ],
   "source": [
    "# Load the parquet dataset\n",
    "if DEV:\n",
    "    # Load DEV dataset\n",
    "    df = load_DEV_preprocessed_dataset(spark)\n",
    "else:\n",
    "    # Load production dataset\n",
    "    df = load_preprocessed_dataset(spark)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Year: integer (nullable = true)\n",
      " |-- Month: integer (nullable = true)\n",
      " |-- DayofMonth: integer (nullable = true)\n",
      " |-- Cancelled: integer (nullable = true)\n",
      "\n",
      "+----+-----+----------+---------+\n",
      "|Year|Month|DayofMonth|Cancelled|\n",
      "+----+-----+----------+---------+\n",
      "|1994|    1|         7|        0|\n",
      "|1994|    1|         8|        0|\n",
      "|1994|    1|        10|        0|\n",
      "|1994|    1|        11|        0|\n",
      "|1994|    1|        12|        0|\n",
      "|1994|    1|        13|        1|\n",
      "|1994|    1|        14|        0|\n",
      "|1994|    1|        15|        0|\n",
      "|1994|    1|        17|        0|\n",
      "|1994|    1|        18|        0|\n",
      "+----+-----+----------+---------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Keep only the dimensions we need\n",
    "df = df.select(df['Year'], df['Month'], df['DayofMonth'], df['Cancelled'])\n",
    "# Explore the data\n",
    "df.printSchema()\n",
    "df.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------------------+\n",
      "|summary|           Cancelled|\n",
      "+-------+--------------------+\n",
      "|  count|             5180048|\n",
      "|   mean|0.012884050495284986|\n",
      "| stddev|  0.1127743507776497|\n",
      "|    min|                   0|\n",
      "|    max|                   1|\n",
      "+-------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.describe('Cancelled').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parse dates to datetime format\n",
    "import datetime\n",
    "import pyspark.sql.functions as F\n",
    "from pyspark.sql.types import TimestampType, IntegerType\n",
    "\n",
    "make_date = lambda year, month, day : datetime.datetime(year, month, day) \n",
    "make_date = F.udf(make_date, TimestampType())\n",
    "\n",
    "\n",
    "#df = df.select(make_date(df['Year'], df['Month'], df['DayofMonth']).alias('Date'), 'Cancelled')\n",
    "df = df.select(df['Year'], df['Month'], df['DayofMonth'], df['Cancelled'])\n",
    "\n",
    "#df.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----+----------+-----+\n",
      "|Year|Month|DayOfMonth|count|\n",
      "+----+-----+----------+-----+\n",
      "|1994|   10|        28|14847|\n",
      "|1994|   12|        26|14751|\n",
      "|1994|    2|        11|14242|\n",
      "|1994|   12|        10|13409|\n",
      "|1994|    4|        13|14450|\n",
      "|1994|    6|        23|14527|\n",
      "|1994|    9|        26|14680|\n",
      "|1994|   12|        11|14158|\n",
      "|1994|   11|        24|11524|\n",
      "|1994|   12|         4|14088|\n",
      "+----+-----+----------+-----+\n",
      "only showing top 10 rows\n",
      "\n",
      "+----+-----+----------+-----+\n",
      "|Year|Month|DayOfMonth|count|\n",
      "+----+-----+----------+-----+\n",
      "|1994|   10|        28|   37|\n",
      "|1994|   12|        26|  159|\n",
      "|1994|    2|        11| 3649|\n",
      "|1994|   12|        10|   80|\n",
      "|1994|    4|        13|  368|\n",
      "|1994|    6|        23|   88|\n",
      "|1994|    9|        26|   59|\n",
      "|1994|   12|        11|   76|\n",
      "|1994|   11|        24|   14|\n",
      "|1994|   12|         4|   80|\n",
      "+----+-----+----------+-----+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Cancelled Flights\n",
    "cancelled_flights = df.filter(df['Cancelled'] == 1)\n",
    "\n",
    "# Number of flights per day \n",
    "all_flights_count = df.groupBy(['Year', 'Month', 'DayOfMonth']).count()\n",
    "cancelled_flights_count = cancelled_flights.groupBy(['Year', 'Month', 'DayOfMonth']).count()\n",
    "\n",
    "all_flights_count.show(10)\n",
    "cancelled_flights_count.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nall_flights_count_dataset = '../dataset/all_flights_count_dataset.parquet'\\n\\npath= Path(all_flights_count_dataset)\\nif not path.is_dir():\\n    all_flights_count.write.mode('overwrite').save(all_flights_count_dataset, format='parquet')\\n\\nall_flights_count = spark.read.load(all_flights_count_dataset)\\n    \\n# Store output Dataframe (or load it if already existing)\\ncancelled_flights_count_dataset = '../dataset/cancelled_flights_count_dataset.parquet'\\n\\npath= Path(cancelled_flights_count_dataset)\\nif not path.is_dir():\\n    cancelled_flights_count.write.mode('overwrite').save(cancelled_flights_count_dataset, format='parquet')\\ncancelled_flights_count = spark.read.load(cancelled_flights_count_dataset)\\n\""
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Store output Dataframe (or load it if already existing)\n",
    "'''\n",
    "all_flights_count_dataset = '../dataset/all_flights_count_dataset.parquet'\n",
    "\n",
    "path= Path(all_flights_count_dataset)\n",
    "if not path.is_dir():\n",
    "    all_flights_count.write.mode('overwrite').save(all_flights_count_dataset, format='parquet')\n",
    "\n",
    "all_flights_count = spark.read.load(all_flights_count_dataset)\n",
    "    \n",
    "# Store output Dataframe (or load it if already existing)\n",
    "cancelled_flights_count_dataset = '../dataset/cancelled_flights_count_dataset.parquet'\n",
    "\n",
    "path= Path(cancelled_flights_count_dataset)\n",
    "if not path.is_dir():\n",
    "    cancelled_flights_count.write.mode('overwrite').save(cancelled_flights_count_dataset, format='parquet')\n",
    "cancelled_flights_count = spark.read.load(cancelled_flights_count_dataset)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Rename count columns\n",
    "all_flights_count = all_flights_count.select('Year', 'Month', 'DayOfMonth', all_flights_count['count'].alias('total_count'))\n",
    "cancelled_flights_count = cancelled_flights_count.select('Year', 'Month', 'DayOfMonth', cancelled_flights_count['count'].alias('canceled_count'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----+----------+-----------+--------------+\n",
      "|Year|Month|DayOfMonth|total_count|canceled_count|\n",
      "+----+-----+----------+-----------+--------------+\n",
      "|1994|   10|        28|      14847|            37|\n",
      "|1994|   12|        26|      14751|           159|\n",
      "|1994|    2|        11|      14242|          3649|\n",
      "|1994|   12|        10|      13409|            80|\n",
      "|1994|    4|        13|      14450|           368|\n",
      "|1994|    6|        23|      14527|            88|\n",
      "|1994|    9|        26|      14680|            59|\n",
      "|1994|   12|        11|      14158|            76|\n",
      "|1994|   11|        24|      11524|            14|\n",
      "|1994|   12|         4|      14088|            80|\n",
      "+----+-----+----------+-----------+--------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Join the tables\n",
    "unified_dataset = all_flights_count \\\n",
    "                .join(cancelled_flights_count, ['Year', 'Month', 'DayOfMonth'])\n",
    "    \n",
    "unified_dataset.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----+----------+-----------+--------------+------------------------------+\n",
      "|Year|Month|DayOfMonth|total_count|canceled_count|DailyCanceledFlightsPercentage|\n",
      "+----+-----+----------+-----------+--------------+------------------------------+\n",
      "|1994|   10|        28|      14847|            37|           0.24920859432882064|\n",
      "|1994|   12|        26|      14751|           159|             1.077893024201749|\n",
      "|1994|    2|        11|      14242|          3649|             25.62140148855498|\n",
      "|1994|   12|        10|      13409|            80|            0.5966142143336565|\n",
      "|1994|    4|        13|      14450|           368|             2.546712802768166|\n",
      "|1994|    6|        23|      14527|            88|             0.605768568871756|\n",
      "|1994|    9|        26|      14680|            59|           0.40190735694822893|\n",
      "|1994|   12|        11|      14158|            76|            0.5367989829071903|\n",
      "|1994|   11|        24|      11524|            14|           0.12148559527941688|\n",
      "|1994|   12|         4|      14088|            80|            0.5678591709256104|\n",
      "+----+-----+----------+-----------+--------------+------------------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "unified_dataset = unified_dataset.withColumn(\"DailyCanceledFlightsPercentage\", (F.col(\"canceled_count\") / F.col(\"total_count\"))* 100)\n",
    "unified_dataset.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "unified_dataset = unified_dataset.select('Year', 'Month', 'DayOfMonth', 'DailyCanceledFlightsPercentage')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store output Dataframe (or load it if already existing)\n",
    "final_dataset = '../dataset/canceled_analitics.parquet'\n",
    "\n",
    "path= Path(final_dataset)\n",
    "if not path.is_dir():\n",
    "    unified_dataset.write.mode('overwrite').save(final_dataset, format='parquet')\n",
    "unified_dataset = spark.read.load(final_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(1994, 4, 19, 0.3580773998071891), (1994, 4, 27, 1.6102280580511403), (1994, 4, 3, 0.5050135402181073), (1994, 10, 23, 0.2274827610720125), (1994, 10, 30, 0.1984689537850865), (1994, 11, 14, 0.46694186912093116), (1994, 2, 3, 0.7247906551263106), (1994, 3, 1, 1.5824581322736304), (1994, 3, 19, 0.5396949550254204), (1994, 6, 1, 0.25403364229316855), (1994, 9, 3, 0.20145668681233533), (1994, 2, 1, 0.6602768903088392), (1994, 5, 20, 0.19831771866238118), (1994, 7, 25, 0.9267461669505962), (1994, 8, 16, 0.35321287868496126), (1994, 10, 10, 0.24772362078200322), (1994, 10, 16, 0.06376647300552643), (1994, 2, 10, 8.757021460463775), (1994, 3, 31, 0.19327673086215227), (1994, 6, 20, 0.71280276816609), (1994, 8, 28, 0.30164854437039634), (1994, 10, 25, 0.2633889376646181), (1994, 11, 10, 0.7276931447225246), (1994, 4, 11, 0.9873258175197815), (1994, 5, 6, 0.2835604122000138), (1994, 5, 11, 0.2623584645125656), (1994, 1, 20, 3.1078742428612633), (1994, 6, 6, 0.5723348503654668), (1994, 6, 9, 0.18612987729215497), (1994, 9, 23, 0.649395037254768), (1994, 10, 31, 1.561974011984111), (1994, 1, 13, 2.2400796472763473), (1994, 4, 24, 0.16104238342727473), (1994, 4, 28, 0.8632692843219159), (1994, 5, 12, 0.4978564513898493), (1994, 8, 8, 0.29016802753222215), (1994, 3, 15, 0.3091933489075168), (1994, 7, 9, 0.9989716468341413), (1994, 11, 5, 1.1867207450803665), (1994, 11, 26, 0.2364066193853428), (1994, 12, 18, 0.5134336756224503), (1994, 1, 5, 4.765613935703159), (1994, 2, 6, 0.5899484728549026), (1994, 2, 21, 2.5163491612169464), (1994, 7, 27, 0.6939247567861759), (1994, 10, 12, 0.24813895781637718), (1994, 1, 28, 6.977409207892479), (1994, 5, 2, 0.8266759291420632), (1994, 5, 13, 0.2844654131686672), (1994, 8, 19, 0.570690943678239), (1994, 9, 29, 0.25159798721610227), (1994, 2, 15, 0.30128923766816146), (1994, 5, 3, 0.44957808825563705), (1994, 10, 22, 0.60084563459684), (1994, 12, 9, 0.8452048258469088), (1994, 12, 25, 0.6304483188044832), (1994, 1, 29, 4.325963764067364), (1994, 3, 2, 6.069770713180475), (1994, 6, 14, 0.4364694471387003), (1994, 11, 20, 0.45800450958286354), (1994, 12, 24, 0.6402748496915749), (1994, 3, 29, 0.5116857972617895), (1994, 3, 5, 0.4818902533810042), (1994, 10, 13, 0.3408863043914177), (1994, 7, 18, 0.8628889794809077), (1994, 11, 7, 0.41969222570115244), (1994, 11, 17, 0.7075792624846918), (1994, 1, 1, 0.5264023688106596), (1994, 1, 26, 5.9510086455331415), (1994, 3, 11, 0.4080785724166551), (1994, 8, 15, 0.7453584496544247), (1994, 12, 14, 0.9224716814759547), (1994, 12, 28, 0.564023366682334), (1994, 5, 19, 0.20821765685730148), (1994, 5, 15, 0.1464343242055938), (1994, 7, 8, 1.6365379348123803), (1994, 1, 15, 1.20414673046252), (1994, 2, 16, 0.3561701236119841), (1994, 12, 16, 0.6005596123660684), (1994, 5, 17, 0.4822929585228055), (1994, 9, 20, 0.2580995720980779), (1994, 9, 7, 0.2483721554675438), (1994, 5, 9, 0.25584289863089477), (1994, 8, 22, 1.2693082605775687), (1994, 9, 9, 0.5226007872946925), (1994, 10, 6, 0.37657185125411874), (1994, 3, 28, 0.9745231797299179), (1994, 7, 13, 0.7550506768247058), (1994, 10, 14, 0.18916362653695445), (1994, 12, 15, 0.7499318243796018), (1994, 4, 13, 2.546712802768166), (1994, 6, 23, 0.605768568871756), (1994, 9, 26, 0.40190735694822893), (1994, 12, 11, 0.5367989829071903), (1994, 4, 4, 0.35414207346712034), (1994, 6, 18, 0.4199160167966407), (1994, 7, 30, 0.7994132746607994), (1994, 12, 23, 0.5168132089401973), (1994, 1, 17, 17.112681081852564), (1994, 7, 29, 0.4489185144878248)]\n"
     ]
    }
   ],
   "source": [
    "# Output a list of tuples of schema:\n",
    "# ('Data', 'Percentage')\n",
    "cancel_data = unified_dataset.rdd.map(tuple).collect()\n",
    "print(cancel_data[:100])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hide warnings if there are any\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "%matplotlib ipympl\n",
    "\n",
    "from __future__ import print_function\n",
    "from ipywidgets import interact, interactive, fixed, interact_manual\n",
    "import ipywidgets as widgets\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pd_dataframe(years, df):\n",
    "    rows = df.filter(F.col('Year').isin(*years)) \\\n",
    "             .select('Year', 'Month', 'DayOfMonth', 'DailyCanceledFlightsPercentage') \\\n",
    "             .orderBy('Year', 'Month', 'DayOfMonth') \\\n",
    "             .collect()\n",
    "    \n",
    "    nb_years = len(years)\n",
    "    nb_days = 365\n",
    "    data = np.zeros((nb_days, nb_years))\n",
    "    for row in rows:\n",
    "        year = row[0] - years[0]\n",
    "        date = pd.to_datetime(str(row[0])+str(row[1])+str(row[2]), format='%Y%m%d')\n",
    "        new_year_day = pd.Timestamp(year=date.year, month=1, day=1)\n",
    "        day = (date - new_year_day).days + 1\n",
    "        pen = row[3]\n",
    "\n",
    "        if day > 364: continue\n",
    "        data[day, year] = pen\n",
    "    columns = [str(y) for y in years]\n",
    "    indices = range(1, 366)\n",
    "    res = pd.DataFrame(data=data, columns=columns, index=indices)\n",
    "    return res\n",
    "#Check sul controllo di anni bisestili\n",
    "\n",
    "def plot_canceled_time_series(date, df,ax):\n",
    "    df = get_pd_dataframe(date, df)\n",
    "    title = 'Daily canceled flights percentage'\n",
    "    if df.empty:\n",
    "        print('No data')\n",
    "    else:\n",
    "        df.plot(title=title, grid=True, xticks=range(0, 53, 4), ax=ax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_average_df(years, df):\n",
    "    rows = df.filter(F.col('Year').isin(*years)) \\\n",
    "             .groupBy('Year') \\\n",
    "             .avg('DailyCanceledFlightsPercentage') \\\n",
    "             .withColumnRenamed('avg(DailyCanceledFlightsPercentage)', 'AverageDailyCanceledFlightsPercentage') \\\n",
    "             .select('Year', 'AverageDailyCanceledFlightsPercentage') \\\n",
    "             .collect()\n",
    "    \n",
    "    nb_years = len(years)\n",
    "    data = np.zeros(nb_years)\n",
    "    for row in rows:\n",
    "        year = row[0] - years[0]\n",
    "        avg_pen = row[1]\n",
    "        data[year] = avg_pen \n",
    "    res = pd.DataFrame({'Canceled flights': data}, index=years)\n",
    "    return res\n",
    "\n",
    "def plot_average_canceled_flights(years, df, ax):\n",
    "    df = get_average_df( years, df)\n",
    "    title = 'Average canceled flights percentage'\n",
    "    if df.empty:\n",
    "        print('No data')\n",
    "    else:\n",
    "        df.plot.bar(title=title, rot=0 , ax=ax)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ui_callback( years, df):\n",
    "    plt.figure(figsize=(15,12))\n",
    "    plt.clf()\n",
    "    ax = plt.subplot(211)\n",
    "    plot_canceled_time_series(range(years[0], years[1] + 1), df, ax)\n",
    "    \n",
    "    ax = plt.subplot(212)\n",
    "    plot_average_canceled_flights(range(years[0], years[1] + 1), df, ax)\n",
    "    \n",
    "    plt.subplots_adjust(top=0.92, bottom=0.08, left=0.10, right=0.95, hspace=0.25,\n",
    "                    wspace=0.35)\n",
    "    plt.show()\n",
    "\n",
    "# Years selection range\n",
    "years = range(1994, 2009)\n",
    "years = [(str(y), y) for y in years]\n",
    "years_w = widgets.SelectionRangeSlider(options=years,\n",
    "                                       index=(0, 2),\n",
    "                                       description='Years',\n",
    "                                       continuous_update=False)\n",
    "ui = widgets.HBox([years_w])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "152bfab20056467dbc40ad42043da67d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FigureCanvasNbAgg()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8c8e8a82b5844f7baadccdfd5bc46c96",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(SelectionRangeSlider(continuous_update=False, description='Years', index=(0, 2), options=(('199â€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a8be748a693940a99c08dffb17574ca2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "out = widgets.interactive_output(ui_callback, {'years': years_w, 'df': widgets.fixed(unified_dataset)})\n",
    "display(ui, out)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
