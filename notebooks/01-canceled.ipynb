{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize PySpark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find Apache Spark on this machine\n",
    "import findspark\n",
    "findspark.init('/Users/giacomogregori/spark')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Dev mode: False when performing real analytics\n",
    "DEV = True\n",
    "\n",
    "# Build a Spark SQL Session for DataFrames\n",
    "master = 'local[2]'\n",
    "appName = 'Cancelled flights percentages'\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(appName) \\\n",
    "    .master(master) \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------- DEV mode ON ---------\n",
      "Starting preprocessing of ../dataset/1994.csv.bz2\n",
      "Preprocessing NOT performed.\n",
      "Preprocessed dataset already exists: ../dataset/preprocessed_dataset_1994.parquet\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from preprocessing_utils import *\n",
    "if DEV:\n",
    "    # DEV preprocessing\n",
    "    perform_DEV_dataset_preprocessing(spark)\n",
    "else:\n",
    "    # Production preprocessing\n",
    "    perform_dataset_preprocessing(spark)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------- DEV mode ON ---------\n",
      "Peprocessed dataset loaded.\n",
      "../dataset/preprocessed_dataset_1994.parquet\n"
     ]
    }
   ],
   "source": [
    "# Load the parquet dataset\n",
    "if DEV:\n",
    "    # Load DEV dataset\n",
    "    df = load_DEV_preprocessed_dataset(spark)\n",
    "else:\n",
    "    # Load production dataset\n",
    "    df = load_preprocessed_dataset(spark)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Year: integer (nullable = true)\n",
      " |-- Month: integer (nullable = true)\n",
      " |-- DayofMonth: integer (nullable = true)\n",
      " |-- Cancelled: integer (nullable = true)\n",
      "\n",
      "+----+-----+----------+---------+\n",
      "|Year|Month|DayofMonth|Cancelled|\n",
      "+----+-----+----------+---------+\n",
      "|1994|    1|         7|        0|\n",
      "|1994|    1|         8|        0|\n",
      "|1994|    1|        10|        0|\n",
      "|1994|    1|        11|        0|\n",
      "|1994|    1|        12|        0|\n",
      "|1994|    1|        13|        1|\n",
      "|1994|    1|        14|        0|\n",
      "|1994|    1|        15|        0|\n",
      "|1994|    1|        17|        0|\n",
      "|1994|    1|        18|        0|\n",
      "+----+-----+----------+---------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Keep only the dimensions we need\n",
    "df = df.select(df['Year'], df['Month'], df['DayofMonth'], df['Cancelled'])\n",
    "# Explore the data\n",
    "df.printSchema()\n",
    "df.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------------------+\n",
      "|summary|           Cancelled|\n",
      "+-------+--------------------+\n",
      "|  count|             5180048|\n",
      "|   mean|0.012884050495284986|\n",
      "| stddev|  0.1127743507776497|\n",
      "|    min|                   0|\n",
      "|    max|                   1|\n",
      "+-------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.describe('Cancelled').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parse dates to datetime format\n",
    "import datetime\n",
    "import pyspark.sql.functions as F\n",
    "from pyspark.sql.types import TimestampType, IntegerType\n",
    "\n",
    "make_date = lambda year, month, day : datetime.datetime(year, month, day) \n",
    "make_date = F.udf(make_date, TimestampType())\n",
    "\n",
    "\n",
    "df = df.select(make_date(df['Year'], df['Month'], df['DayofMonth']).alias('Date'), 'Cancelled')\n",
    "#df.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+-----+\n",
      "|               Date|count|\n",
      "+-------------------+-----+\n",
      "|1994-10-26 00:00:00|14838|\n",
      "|1994-10-20 00:00:00|14800|\n",
      "|1994-12-02 00:00:00|14815|\n",
      "|1994-07-09 00:00:00|13614|\n",
      "|1994-12-16 00:00:00|14653|\n",
      "|1994-02-24 00:00:00|14121|\n",
      "|1994-12-01 00:00:00|14510|\n",
      "|1994-02-07 00:00:00|14126|\n",
      "|1994-01-19 00:00:00|13776|\n",
      "|1994-04-25 00:00:00|14389|\n",
      "+-------------------+-----+\n",
      "only showing top 10 rows\n",
      "\n",
      "+-------------------+-----+\n",
      "|               Date|count|\n",
      "+-------------------+-----+\n",
      "|1994-10-26 00:00:00|   36|\n",
      "|1994-10-20 00:00:00|  212|\n",
      "|1994-12-02 00:00:00|   99|\n",
      "|1994-07-09 00:00:00|  136|\n",
      "|1994-12-16 00:00:00|   88|\n",
      "|1994-02-24 00:00:00|  471|\n",
      "|1994-12-01 00:00:00|  163|\n",
      "|1994-02-07 00:00:00|  213|\n",
      "|1994-01-19 00:00:00|  973|\n",
      "|1994-04-25 00:00:00|  122|\n",
      "+-------------------+-----+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Cancelled Flights\n",
    "cancelled_flights = df.filter(df['Cancelled'] == 1)\n",
    "\n",
    "# Number of flights per day \n",
    "all_flights_count = df.groupBy(['Date']).count()\n",
    "cancelled_flights_count = cancelled_flights.groupBy(['Date']).count()\n",
    "\n",
    "all_flights_count.show(10)\n",
    "cancelled_flights_count.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store output Dataframe (or load it if already existing)\n",
    "all_flights_count_dataset = '../dataset/all_flights_count_dataset.parquet'\n",
    "\n",
    "path= Path(all_flights_count_dataset)\n",
    "if not path.is_dir():\n",
    "    all_flights_count.write.mode('overwrite').save(all_flights_count_dataset, format='parquet')\n",
    "else:\n",
    "    all_flights_count = spark.read.load(all_flights_count_dataset)\n",
    "    \n",
    "# Store output Dataframe (or load it if already existing)\n",
    "cancelled_flights_count_dataset = '../dataset/cancelled_flights_count_dataset.parquet'\n",
    "\n",
    "path= Path(cancelled_flights_count_dataset)\n",
    "if not path.is_dir():\n",
    "    cancelled_flights_count.write.mode('overwrite').save(cancelled_flights_count_dataset, format='parquet')\n",
    "else:\n",
    "    cancelled_flights_count = spark.read.load(cancelled_flights_count_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Rename count columns\n",
    "all_flights_count = all_flights_count.select('Date', all_flights_count['count'].alias('total_count'))\n",
    "cancelled_flights_count = cancelled_flights_count.select('Date', cancelled_flights_count['count'].alias('canceled_count'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+-----------+--------------+\n",
      "|               Date|total_count|canceled_count|\n",
      "+-------------------+-----------+--------------+\n",
      "|1994-01-15 00:00:00|      12540|           151|\n",
      "|1994-01-16 00:00:00|      13013|           623|\n",
      "|1994-04-21 00:00:00|      14299|            28|\n",
      "|1994-02-06 00:00:00|      13391|            79|\n",
      "|1994-02-11 00:00:00|      14242|          3649|\n",
      "|1994-07-04 00:00:00|      13844|            62|\n",
      "|1994-08-12 00:00:00|      14722|            36|\n",
      "|1994-02-05 00:00:00|      12687|            86|\n",
      "|1994-03-15 00:00:00|      14554|            45|\n",
      "|1994-07-17 00:00:00|      14160|            28|\n",
      "+-------------------+-----------+--------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Join the tables\n",
    "unified_dataset = all_flights_count \\\n",
    "                .join(cancelled_flights_count, [\"Date\"])\n",
    "    \n",
    "unified_dataset.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+-----------+--------------+------------------------------+\n",
      "|               Date|total_count|canceled_count|DailyCanceledFlightsPercentage|\n",
      "+-------------------+-----------+--------------+------------------------------+\n",
      "|1994-01-15 00:00:00|      12540|           151|              1.20414673046252|\n",
      "|1994-01-16 00:00:00|      13013|           623|             4.787520172135556|\n",
      "|1994-04-21 00:00:00|      14299|            28|           0.19581788936289252|\n",
      "|1994-02-06 00:00:00|      13391|            79|            0.5899484728549026|\n",
      "|1994-02-11 00:00:00|      14242|          3649|             25.62140148855498|\n",
      "|1994-07-04 00:00:00|      13844|            62|            0.4478474429355677|\n",
      "|1994-08-12 00:00:00|      14722|            36|           0.24453199293574243|\n",
      "|1994-02-05 00:00:00|      12687|            86|             0.677859225979349|\n",
      "|1994-03-15 00:00:00|      14554|            45|            0.3091933489075168|\n",
      "|1994-07-17 00:00:00|      14160|            28|           0.19774011299435026|\n",
      "+-------------------+-----------+--------------+------------------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "unified_dataset = unified_dataset.withColumn(\"DailyCanceledFlightsPercentage\", (F.col(\"canceled_count\") / F.col(\"total_count\"))* 100)\n",
    "unified_dataset.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "unified_dataset = unified_dataset.select('Date', 'DailyCanceledFlightsPercentage')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store output Dataframe (or load it if already existing)\n",
    "final_dataset = '../dataset/canceled_analitics.parquet'\n",
    "\n",
    "path= Path(final_dataset)\n",
    "if not path.is_dir():\n",
    "    unified_dataset.write.mode('overwrite').save(final_dataset, format='parquet')\n",
    "else:\n",
    "    unified_dataset = spark.read.load(final_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(datetime.datetime(1994, 1, 15, 0, 0), 1.20414673046252), (datetime.datetime(1994, 1, 16, 0, 0), 4.787520172135556), (datetime.datetime(1994, 4, 21, 0, 0), 0.19581788936289252), (datetime.datetime(1994, 2, 6, 0, 0), 0.5899484728549026), (datetime.datetime(1994, 2, 11, 0, 0), 25.62140148855498), (datetime.datetime(1994, 7, 4, 0, 0), 0.4478474429355677), (datetime.datetime(1994, 8, 12, 0, 0), 0.24453199293574243), (datetime.datetime(1994, 2, 5, 0, 0), 0.677859225979349), (datetime.datetime(1994, 3, 15, 0, 0), 0.3091933489075168), (datetime.datetime(1994, 7, 17, 0, 0), 0.19774011299435026), (datetime.datetime(1994, 10, 28, 0, 0), 0.24920859432882064), (datetime.datetime(1994, 11, 24, 0, 0), 0.12148559527941688), (datetime.datetime(1994, 12, 10, 0, 0), 0.5966142143336565), (datetime.datetime(1994, 2, 14, 0, 0), 0.937742367623211), (datetime.datetime(1994, 6, 13, 0, 0), 0.9539725645846389), (datetime.datetime(1994, 6, 15, 0, 0), 0.44778175806007164), (datetime.datetime(1994, 8, 11, 0, 0), 0.3987025273685633), (datetime.datetime(1994, 11, 28, 0, 0), 1.5544397826535525), (datetime.datetime(1994, 12, 29, 0, 0), 0.3618817852834741), (datetime.datetime(1994, 1, 12, 0, 0), 7.870958633347451), (datetime.datetime(1994, 1, 17, 0, 0), 17.112681081852564), (datetime.datetime(1994, 4, 1, 0, 0), 0.20645516482003992), (datetime.datetime(1994, 5, 23, 0, 0), 0.3438553056873668), (datetime.datetime(1994, 5, 14, 0, 0), 0.2873340063679428), (datetime.datetime(1994, 2, 1, 0, 0), 0.6602768903088392), (datetime.datetime(1994, 3, 24, 0, 0), 0.5014625992478061), (datetime.datetime(1994, 7, 29, 0, 0), 0.4489185144878248), (datetime.datetime(1994, 11, 2, 0, 0), 0.8297644539614561), (datetime.datetime(1994, 11, 13, 0, 0), 0.2611703254041082), (datetime.datetime(1994, 1, 22, 0, 0), 0.9481668773704173), (datetime.datetime(1994, 1, 25, 0, 0), 4.026369887289998), (datetime.datetime(1994, 6, 9, 0, 0), 0.18612987729215497), (datetime.datetime(1994, 9, 2, 0, 0), 0.1763548802821678), (datetime.datetime(1994, 12, 8, 0, 0), 0.8653326122228232), (datetime.datetime(1994, 7, 2, 0, 0), 0.4125286478227655), (datetime.datetime(1994, 10, 1, 0, 0), 0.18000450011250282), (datetime.datetime(1994, 10, 3, 0, 0), 0.2379495546944048), (datetime.datetime(1994, 8, 19, 0, 0), 0.570690943678239), (datetime.datetime(1994, 11, 17, 0, 0), 0.7075792624846918), (datetime.datetime(1994, 4, 8, 0, 0), 0.17185674022135147), (datetime.datetime(1994, 3, 13, 0, 0), 0.40301897853007984), (datetime.datetime(1994, 8, 23, 0, 0), 0.47763202152707707), (datetime.datetime(1994, 9, 10, 0, 0), 0.45312732134898237), (datetime.datetime(1994, 12, 28, 0, 0), 0.564023366682334), (datetime.datetime(1994, 1, 26, 0, 0), 5.9510086455331415), (datetime.datetime(1994, 5, 18, 0, 0), 0.35822540644805734), (datetime.datetime(1994, 6, 10, 0, 0), 0.23009343187839912), (datetime.datetime(1994, 10, 24, 0, 0), 0.6154054236829648), (datetime.datetime(1994, 11, 19, 0, 0), 0.4868692829743287), (datetime.datetime(1994, 3, 28, 0, 0), 0.9745231797299179), (datetime.datetime(1994, 4, 10, 0, 0), 0.2799882110226938), (datetime.datetime(1994, 5, 29, 0, 0), 0.278263634918111), (datetime.datetime(1994, 10, 2, 0, 0), 0.22252530328045367), (datetime.datetime(1994, 11, 20, 0, 0), 0.45800450958286354), (datetime.datetime(1994, 3, 30, 0, 0), 0.3447087211306446), (datetime.datetime(1994, 3, 16, 0, 0), 0.40889874558181444), (datetime.datetime(1994, 6, 4, 0, 0), 0.22112085398398781), (datetime.datetime(1994, 7, 14, 0, 0), 1.7633442265795207), (datetime.datetime(1994, 8, 7, 0, 0), 0.6651570902915369), (datetime.datetime(1994, 4, 27, 0, 0), 1.6102280580511403), (datetime.datetime(1994, 2, 4, 0, 0), 0.46669495120916415), (datetime.datetime(1994, 4, 14, 0, 0), 0.4741319202342769), (datetime.datetime(1994, 5, 7, 0, 0), 0.3472222222222222), (datetime.datetime(1994, 1, 29, 0, 0), 4.325963764067364), (datetime.datetime(1994, 5, 12, 0, 0), 0.4978564513898493), (datetime.datetime(1994, 9, 29, 0, 0), 0.25159798721610227), (datetime.datetime(1994, 12, 24, 0, 0), 0.6402748496915749), (datetime.datetime(1994, 1, 5, 0, 0), 4.765613935703159), (datetime.datetime(1994, 5, 6, 0, 0), 0.2835604122000138), (datetime.datetime(1994, 10, 5, 0, 0), 0.22686328151064256), (datetime.datetime(1994, 8, 29, 0, 0), 0.27426583717974445), (datetime.datetime(1994, 5, 16, 0, 0), 0.5455047645352852), (datetime.datetime(1994, 10, 31, 0, 0), 1.561974011984111), (datetime.datetime(1994, 8, 6, 0, 0), 0.235536581775357), (datetime.datetime(1994, 12, 15, 0, 0), 0.7499318243796018), (datetime.datetime(1994, 4, 19, 0, 0), 0.3580773998071891), (datetime.datetime(1994, 4, 30, 0, 0), 0.6290285004271181), (datetime.datetime(1994, 2, 2, 0, 0), 0.5858685677984047), (datetime.datetime(1994, 9, 16, 0, 0), 0.3195757122458693), (datetime.datetime(1994, 1, 6, 0, 0), 5.416755640970887), (datetime.datetime(1994, 1, 23, 0, 0), 1.1328681821592017), (datetime.datetime(1994, 4, 5, 0, 0), 0.6275712990725891), (datetime.datetime(1994, 10, 17, 0, 0), 0.671915297950319), (datetime.datetime(1994, 4, 9, 0, 0), 0.13909280581098832), (datetime.datetime(1994, 5, 30, 0, 0), 0.18706381754083026), (datetime.datetime(1994, 10, 29, 0, 0), 0.16312004152146511), (datetime.datetime(1994, 12, 31, 0, 0), 0.992063492063492), (datetime.datetime(1994, 1, 10, 0, 0), 1.8481801444554598), (datetime.datetime(1994, 4, 20, 0, 0), 0.28551532033426186), (datetime.datetime(1994, 7, 5, 0, 0), 0.682407533779173), (datetime.datetime(1994, 11, 9, 0, 0), 0.8381776395836149), (datetime.datetime(1994, 1, 31, 0, 0), 1.0229276895943562), (datetime.datetime(1994, 8, 8, 0, 0), 0.29016802753222215), (datetime.datetime(1994, 8, 18, 0, 0), 0.19649027711904601), (datetime.datetime(1994, 9, 19, 0, 0), 0.3128187691261476), (datetime.datetime(1994, 2, 16, 0, 0), 0.3561701236119841), (datetime.datetime(1994, 2, 17, 0, 0), 0.39428289797930016), (datetime.datetime(1994, 8, 16, 0, 0), 0.35321287868496126), (datetime.datetime(1994, 9, 7, 0, 0), 0.2483721554675438), (datetime.datetime(1994, 5, 21, 0, 0), 0.32749428789032753)]\n"
     ]
    }
   ],
   "source": [
    "# Output a list of tuples of schema:\n",
    "# ('Data', 'Percentage')\n",
    "cancel_data = unified_dataset.rdd.map(tuple).collect()\n",
    "print(cancel_data[:100])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hide warnings if there are any\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "%matplotlib ipympl\n",
    "\n",
    "from __future__ import print_function\n",
    "from ipywidgets import interact, interactive, fixed, interact_manual\n",
    "import ipywidgets as widgets\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pd_dataframe(years, df):\n",
    "    rows = df.filter(F.col('Date').date.year.isin(*years)) \\\n",
    "             .select('Date', 'DailyCanceledFlightsPercentage') \\\n",
    "             .orderBy('Datw') \\\n",
    "             .collect()\n",
    "    \n",
    "    nb_years = len(years)\n",
    "    nb_days = 365\n",
    "    data = np.zeros((nb_days, nb_years))\n",
    "    for row in rows:\n",
    "        year = row[0].date.year - years[0]\n",
    "        day = row[0].date.timetuple().yday - 1\n",
    "        \n",
    "        pen = row[1]\n",
    "\n",
    "        if day > 364: continue\n",
    "        data[day, year] = pen\n",
    "    columns = [str(y) for y in years]\n",
    "    indices = range(1, 366)\n",
    "    res = pd.DataFrame(data=data, columns=columns, index=indices)\n",
    "    return res\n",
    "#Check sul controllo di anni bisestili\n",
    "\n",
    "def plot_canceled_time_series(date, df):\n",
    "    df = get_pd_dataframe(date, df)\n",
    "    title = 'Weekly canceled flights percentage'\n",
    "    if df.empty:\n",
    "        print('No data')\n",
    "    else:\n",
    "        df.plot(title=title, grid=True, xticks=range(0, 53, 4), colormap='tab20c')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\ndef get_average_df(years, df):\\n    rows = df.filter(F.col('Year').isin(*years))              .groupBy('Year')              .avg('WeeklyWeatherDelays')              .withColumnRenamed('avg(WeeklyWeatherDelays)', 'AverageWeatherDelaysPercentage')              .select('Year', 'AverageWeatherDelaysPercentage')              .collect()\\n    \\n    nb_years = len(years)\\n    data = np.zeros(nb_years)\\n    for row in rows:\\n        year = row[0] - years[0]\\n        avg_pen = row[1]\\n        data[year] = avg_pen \\n    res = pd.DataFrame({'Weather delays': data}, index=years)\\n    return res\\n\\ndef plot_average_canceled_flights(years, df):\\n    df = get_average_df( years, df)\\n    title = 'Average canceled flights percentage'\\n    if df.empty:\\n        print('No data ')\\n    else:\\n        df.plot.bar( title=title, rot=0)\\n\""
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "def get_average_df(years, df):\n",
    "    rows = df.filter(F.col('Year').isin(*years)) \\\n",
    "             .groupBy('Year') \\\n",
    "             .avg('WeeklyWeatherDelays') \\\n",
    "             .withColumnRenamed('avg(WeeklyWeatherDelays)', 'AverageWeatherDelaysPercentage') \\\n",
    "             .select('Year', 'AverageWeatherDelaysPercentage') \\\n",
    "             .collect()\n",
    "    \n",
    "    nb_years = len(years)\n",
    "    data = np.zeros(nb_years)\n",
    "    for row in rows:\n",
    "        year = row[0] - years[0]\n",
    "        avg_pen = row[1]\n",
    "        data[year] = avg_pen \n",
    "    res = pd.DataFrame({'Weather delays': data}, index=years)\n",
    "    return res\n",
    "\n",
    "def plot_average_canceled_flights(years, df):\n",
    "    df = get_average_df( years, df)\n",
    "    title = 'Average canceled flights percentage'\n",
    "    if df.empty:\n",
    "        print('No data ')\n",
    "    else:\n",
    "        df.plot.bar( title=title, rot=0)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ui_callback( years, df):\n",
    "    plot_canceled_time_series(range(years[0], years[1] + 1), df)\n",
    "    # plot_average_canceled_flights(range(years[0], years[1] + 1), df)\n",
    "\n",
    "# Years selection range\n",
    "years = range(1994, 2009)\n",
    "years = [(str(y), y) for y in years]\n",
    "years_w = widgets.SelectionRangeSlider(options=years,\n",
    "                                       index=(0, 2),\n",
    "                                       description='Years',\n",
    "                                       continuous_update=False)\n",
    "ui = widgets.HBox([years_w])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "ename": "AnalysisException",
     "evalue": "\"Can't extract value from Date#233: need struct type but got timestamp;\"",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m~/spark/python/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/spark/python/lib/py4j-0.10.7-src.zip/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    327\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 328\u001b[0;31m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[1;32m    329\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o110.filter.\n: org.apache.spark.sql.AnalysisException: Can't extract value from Date#233: need struct type but got timestamp;\n\tat org.apache.spark.sql.catalyst.expressions.ExtractValue$.apply(complexTypeExtractors.scala:73)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveReferences$.org$apache$spark$sql$catalyst$analysis$Analyzer$ResolveReferences$$resolve(Analyzer.scala:829)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveReferences$$anonfun$org$apache$spark$sql$catalyst$analysis$Analyzer$ResolveReferences$$resolve$2.apply(Analyzer.scala:830)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveReferences$$anonfun$org$apache$spark$sql$catalyst$analysis$Analyzer$ResolveReferences$$resolve$2.apply(Analyzer.scala:830)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$4.apply(TreeNode.scala:306)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:187)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.mapChildren(TreeNode.scala:304)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveReferences$.org$apache$spark$sql$catalyst$analysis$Analyzer$ResolveReferences$$resolve(Analyzer.scala:830)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveReferences$$anonfun$org$apache$spark$sql$catalyst$analysis$Analyzer$ResolveReferences$$resolve$2.apply(Analyzer.scala:830)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveReferences$$anonfun$org$apache$spark$sql$catalyst$analysis$Analyzer$ResolveReferences$$resolve$2.apply(Analyzer.scala:830)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$4.apply(TreeNode.scala:306)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:187)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.mapChildren(TreeNode.scala:304)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveReferences$.org$apache$spark$sql$catalyst$analysis$Analyzer$ResolveReferences$$resolve(Analyzer.scala:830)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveReferences$$anonfun$apply$9$$anonfun$applyOrElse$36.apply(Analyzer.scala:891)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveReferences$$anonfun$apply$9$$anonfun$applyOrElse$36.apply(Analyzer.scala:891)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan$$anonfun$1.apply(QueryPlan.scala:107)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan$$anonfun$1.apply(QueryPlan.scala:107)\n\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:70)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan.transformExpression$1(QueryPlan.scala:106)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan.org$apache$spark$sql$catalyst$plans$QueryPlan$$recursiveTransform$1(QueryPlan.scala:118)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan$$anonfun$2.apply(QueryPlan.scala:127)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:187)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan.mapExpressions(QueryPlan.scala:127)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveReferences$$anonfun$apply$9.applyOrElse(Analyzer.scala:891)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveReferences$$anonfun$apply$9.applyOrElse(Analyzer.scala:833)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$transformUp$1.apply(TreeNode.scala:289)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$transformUp$1.apply(TreeNode.scala:289)\n\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:70)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformUp(TreeNode.scala:288)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveReferences$.apply(Analyzer.scala:833)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveReferences$.apply(Analyzer.scala:690)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor$$anonfun$execute$1$$anonfun$apply$1.apply(RuleExecutor.scala:87)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor$$anonfun$execute$1$$anonfun$apply$1.apply(RuleExecutor.scala:84)\n\tat scala.collection.LinearSeqOptimized$class.foldLeft(LinearSeqOptimized.scala:124)\n\tat scala.collection.immutable.List.foldLeft(List.scala:84)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor$$anonfun$execute$1.apply(RuleExecutor.scala:84)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor$$anonfun$execute$1.apply(RuleExecutor.scala:76)\n\tat scala.collection.immutable.List.foreach(List.scala:381)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:76)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.org$apache$spark$sql$catalyst$analysis$Analyzer$$executeSameContext(Analyzer.scala:124)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:118)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:103)\n\tat org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:57)\n\tat org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:55)\n\tat org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:47)\n\tat org.apache.spark.sql.Dataset.<init>(Dataset.scala:172)\n\tat org.apache.spark.sql.Dataset.<init>(Dataset.scala:178)\n\tat org.apache.spark.sql.Dataset$.apply(Dataset.scala:65)\n\tat org.apache.spark.sql.Dataset.withTypedPlan(Dataset.scala:3301)\n\tat org.apache.spark.sql.Dataset.filter(Dataset.scala:1458)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:748)\n",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/ipywidgets/widgets/interaction.py\u001b[0m in \u001b[0;36mobserver\u001b[0;34m(change)\u001b[0m\n\u001b[1;32m     72\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     73\u001b[0m             \u001b[0mclear_output\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 74\u001b[0;31m             \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     75\u001b[0m             \u001b[0mshow_inline_matplotlib_plots\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mw\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mcontrols\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-19-afbbaaa63d0b>\u001b[0m in \u001b[0;36mui_callback\u001b[0;34m(years, df)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mui_callback\u001b[0m\u001b[0;34m(\u001b[0m \u001b[0myears\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0mplot_canceled_time_series\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0myears\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0myears\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m     \u001b[0;31m# plot_average_canceled_flights(range(years[0], years[1] + 1), df)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# Years selection range\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-17-03a6e677f9f2>\u001b[0m in \u001b[0;36mplot_canceled_time_series\u001b[0;34m(date, df)\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mplot_canceled_time_series\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m     \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_pd_dataframe\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m     \u001b[0mtitle\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'Weekly canceled flights percentage'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mempty\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-17-03a6e677f9f2>\u001b[0m in \u001b[0;36mget_pd_dataframe\u001b[0;34m(years, df)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mget_pd_dataframe\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0myears\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0mrows\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfilter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcol\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Date'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdate\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0myear\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0myears\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m              \u001b[0;34m.\u001b[0m\u001b[0mselect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Date'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'DailyCanceledFlightsPercentage'\u001b[0m\u001b[0;34m)\u001b[0m              \u001b[0;34m.\u001b[0m\u001b[0morderBy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Datw'\u001b[0m\u001b[0;34m)\u001b[0m              \u001b[0;34m.\u001b[0m\u001b[0mcollect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mnb_years\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0myears\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mnb_days\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m365\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/spark/python/pyspark/sql/dataframe.py\u001b[0m in \u001b[0;36mfilter\u001b[0;34m(self, condition)\u001b[0m\n\u001b[1;32m   1240\u001b[0m             \u001b[0mjdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfilter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcondition\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1241\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcondition\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mColumn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1242\u001b[0;31m             \u001b[0mjdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfilter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcondition\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1243\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1244\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"condition should be string or Column\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/spark/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1255\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1256\u001b[0m         return_value = get_return_value(\n\u001b[0;32m-> 1257\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m   1258\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1259\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/spark/python/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m     67\u001b[0m                                              e.java_exception.getStackTrace()))\n\u001b[1;32m     68\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'org.apache.spark.sql.AnalysisException: '\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 69\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mAnalysisException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m': '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstackTrace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     70\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'org.apache.spark.sql.catalyst.analysis'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mAnalysisException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m': '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstackTrace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAnalysisException\u001b[0m: \"Can't extract value from Date#233: need struct type but got timestamp;\""
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "22d45b9bcd994868a8d32b1a9d1e7f4d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(SelectionRangeSlider(continuous_update=False, description='Years', index=(0, 2), options=(('199…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "adcab415e6b24f6a97c5b71e180fab7e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "out = widgets.interactive_output(ui_callback, {'years': years_w, 'df': widgets.fixed(unified_dataset)})\n",
    "display(ui, out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
